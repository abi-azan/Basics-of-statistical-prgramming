---
title: "OSP projekt"
author: "36556413 Albert Maršić"
date: "2026-01-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r libraries}
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(tibble)
library(caret)
library(broom)
```

```{r data-load}
data <- read.csv("Speed Dating Data.csv")
summary(data)
glimpse(data)
```

Primječujemo da imamo na prvi pogled velik broj nepoznatih/nepotpunih čelija, to bi nam moglo otežati analizu.

# Dali ljepota utjeće (prividno) na druge kvalitete

Da i se osoba ćini bolja i u drugim stavkama ako je zgodnija
```{r attr effect cor}
zacajke <- c("attr", "fun", "intel", "sinc")
cor_table <- cor(data[zacajke], use = "pairwise.complete.obs") # imamo puno NA pa treba eliminirati nepotpune podatke

attr_table <- tibble(
  trait = c("fun", "intel", "sinc"),
  dostupnih_podataka = c(
    data %>% select(attr, fun) %>% drop_na() %>% nrow(),
    data %>% select(attr, intel) %>% drop_na() %>% nrow(),
    data %>% select(attr, sinc) %>% drop_na() %>% nrow()
  ),
  korelacija = round(cor_table["attr", c("fun", "intel", "sinc")], 3)
)

attr_table
```





```{r attr plot}
attr.x.other <- data %>%
  select(all_of(zacajke)) %>%
  pivot_longer(cols = -attr, names_to = "trait", values_to = "rating") %>%
  drop_na()
attr.x.other$trait = factor(attr.x.other$trait)
ggplot(attr.x.other, aes(x = attr, y = rating)) +
  geom_point(alpha = 0.1, size = 0.7) +
  geom_smooth(method = "lm", se = FALSE, color = "firebrick") +
  facet_wrap(~ trait, ncol = 3) +
  labs(
    title = "Efekt ljepote na percepciju drugih karakteristika"
  )
ggplot(attr.x.other, aes(x = attr, y = rating),color="trait") +
  geom_point(alpha = 0.5,position = "jitter") +
  geom_smooth(method = "lm", color = "red")


```


```{r attr linear model}
attr_model <- lm(rating ~ attr + trait, data = attr.x.other)

summary(attr_model)
```
Uočavamo definitnu linearnu korelaciju između attraktivnoti i ostalih karakterisitika

# Lažu li participatni o tome što im je zapravo važno

Usporedba stavki za koje ljudi tvrde da su im važne protiv tipa ljudi kojeg na kraju izaberu.

## Što ljudi kažu da im je važno

Za svakog sudionika izvucemo prefernce
```{r stated-preferences}
pref_cols <- c("attr1_1", "sinc1_1", "intel1_1", "fun1_1", "amb1_1", "shar1_1")

# One row per participant (preferences are constant per participant)
prefs <- data %>%
  distinct(iid, .keep_all = TRUE) %>%
  select(all_of(pref_cols)) %>%
  drop_na()

# Convert to shares so 1-10 and 100-point scales are comparable
pref_shares <- prefs / rowSums(prefs)

#mean_pref_shares <- sort(colMeans(pref_shares), decreasing = TRUE)
#mean_pref_shares

mean_pref_shares2 <- sort(sapply(pref_shares,mean), decreasing = TRUE)
mean_pref_shares2

```

```{r plot preferceni}
pref_plot <- tibble(
  attribute = names(mean_pref_shares2),
  mean_share = as.numeric(mean_pref_shares2)
)

ggplot(pref_plot, aes(x = attribute, y = mean_share)) +
  geom_col(fill = "lightblue") +
  coord_flip() +
  labs(
    title = "Proprocija važnosti prefernece"
  )
```

## Predikcija izbora

Korelacija
```{r pravi izbori}
choice_attrs <- c("attr", "fun", "intel", "sinc", "amb", "shar")

choice_cor <- sapply(choice_attrs, function(v) {
  cor(data$dec, data[[v]], use = "pairwise.complete.obs")
})

choice_table <- tibble(
  attribute = choice_attrs,
  r = round(choice_cor, 3)
)

choice_table[order(choice_table$r, decreasing = TRUE), ]
```

Atrakcija ,zabava te isti interesi najviše koreliraju s odlukom pa bi se koglo reći da su oni najbitni za odluku. Dok inteligencija ne korelira toliko iako je druga ajpoželjnija karakteristika.

# Logistička regresija za predviđanje dec

## Logistička regresija  
### Možemo li predvidjeti hoće li sudionik reći “da” (`dec`)?


```{r glm dec}
glm_data <- data %>%
  select(dec, attr, intel, fun, sinc,amb,shar) %>%
  mutate(dec = factor(dec, levels = c(0,1))) %>%
  drop_na()

karakteristike_glm <- glm(dec ~ attr + intel + fun + sinc + amb +shar,
             data = glm_data, family = binomial)


tidy(karakteristike_glm, exponentiate = TRUE, conf.int = TRUE)
summary(karakteristike_glm)
```
Opet uviđamo slično kao i u korelaciiji da najvažniji atrakcija, zabava i zajednički interesi.

```{r glm-pred-plot}
# Predicted probability vs attractiveness (others held at mean)
glm_pred_grid <- glm_data %>%
  summarize(
    attr = seq(min(attr, na.rm = TRUE), max(attr, na.rm = TRUE), length.out = 50),
    intel = mean(intel, na.rm = TRUE),
    fun = mean(fun, na.rm = TRUE),
    sinc = mean(sinc, na.rm = TRUE),
    amb = mean(amb, na.rm = TRUE),
    shar = mean(shar, na.rm = TRUE)
  )

glm_pred_grid$pred <- predict(karakteristike_glm, newdata = glm_pred_grid, type = "response")

ggplot(glm_pred_grid, aes(x = attr, y = pred)) +
  geom_line(color = "steelblue", linewidth = 1) +
  labs(
    title = "Predicted Pr(dec = 1) by Attractiveness",
    x = "Attractiveness (attr)",
    y = "Predicted probability"
  )
```

# Calibration bias: prob vs actual dec_o

Usporedba samoprocjene (`prob`) s realnim ishodom (`dec_o`) pokazuje precjenjuju li sudionici šansu da druga osoba kaže “da”.

```{r calibration-bias}
calib_data <- data %>%
  select(prob, dec_o) %>%
  mutate(
    prob = prob / 10
  ) %>%
  drop_na()

# Bin probs into deciles for calibration curve
calib_bins <- calib_data %>%
  mutate(bin = cut(prob, breaks = seq(0, 1, by = 0.1), include.lowest = TRUE)) %>%
  group_by(bin) %>%
  summarize(
    mean_prob = mean(prob),
    actual_rate = mean(dec_o),
    n = n(),
    .groups = "drop"
  )

ggplot(calib_bins, aes(x = mean_prob, y = actual_rate)) +
  geom_point(size = 2) +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") +
  labs(
    title = "Calibration: Expected vs Actual 'Yes' Rate",
    x = "Mean expected probability (prob)",
    y = "Observed 'Yes' rate (dec_o)"
  )
```

```{r calibration-monte-carlo}
set.seed(123)

# Monte Carlo: compare expected vs observed yes-rate under the stated probabilities
mc_trials <- 1000
mc_expected <- numeric(mc_trials)

for (i in seq_len(mc_trials)) {
  sim_yes <- rbinom(n = nrow(calib_data), size = 1, prob = calib_data$prob)
  mc_expected[i] <- mean(sim_yes)
}

observed_rate <- mean(calib_data$dec_o)
expected_rate <- mean(calib_data$prob)

tibble(
  observed_rate = observed_rate,
  expected_rate = expected_rate,
  mc_mean = mean(mc_expected),
  mc_sd = sd(mc_expected)
)

ggplot(tibble(mc_expected = mc_expected), aes(x = mc_expected)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  geom_vline(xintercept = observed_rate, color = "red", linewidth = 1) +
  geom_vline(xintercept = expected_rate, color = "black", linetype = "dashed") +
  labs(
    title = "Monte Carlo: Expected Yes-Rate Distribution",
    x = "Simulated mean yes-rate",
    y = "Count"
  )
```

### funkcija train linearna regresija (prediktivno)


```{r caret dec}
set.seed(123)

ml_lm_data <- data %>%
  select(like, attr, intel, fun, amb, shar) %>%
  drop_na()

idx <- createDataPartition(ml_lm_data$like, p = 0.8, list = FALSE)
train_data <- ml_lm_data[idx, ]
test_data  <- ml_lm_data[-idx, ]

ctrl <- trainControl(method = "cv", number = 5)

fit_lm <- train(like ~ ., data = train_data,
                method = "lm",
                trControl = ctrl)

fit_lm

pred <- predict(fit_lm, newdata = test_data)
postResample(pred = pred, obs = test_data$like)

```
## kNN klasifikacija + confusionMatrix

### Pitanje  
**Može li kNN dobro predvidjeti `match`?**


```{r caret-knn}
set.seed(123)

knn_data <- data %>%
  select(match, attr, intel, fun, like) %>%
  drop_na() %>%
  mutate(match = factor(match, levels = c(0,1)))

idx <- createDataPartition(knn_data$match, p = 0.8, list = FALSE)
train_knn <- knn_data[idx, ]
test_knn  <- knn_data[-idx, ]

# kNN je osjetljiv na skalu -> centriranje i skaliranje
ctrl <- trainControl(method = "cv", number = 5)

fit_knn <- train(match ~ ., data = train_knn,
                 method = "knn",
                 preProcess = c("center", "scale"),
                 tuneLength = 10,
                 trControl = ctrl)

fit_knn

pred_knn <- predict(fit_knn, newdata = test_knn)
confusionMatrix(pred_knn, test_knn$match)

---
```
## Decision Tree (caret + rpart)

### Pitanje  
**Koje varijable najviše “odlučuju” o `match` prema stablu?**



```{r caret-tree}
set.seed(123)
library(rpart)

tree_data <- data %>%
  select(match, attr, intel, fun, like, samerace) %>%
  drop_na() %>%
  mutate(match = factor(match, levels = c(0,1)))

idx <- createDataPartition(tree_data$match, p = 0.8, list = FALSE)
train_tree <- tree_data[idx, ]
test_tree  <- tree_data[-idx, ]

ctrl <- trainControl(method = "cv", number = 5)

fit_tree <- train(match ~ ., data = train_tree,
                  method = "rpart",
                  trControl = ctrl,
                  tuneLength = 10)

fit_tree

pred_tree <- predict(fit_tree, newdata = test_tree)
confusionMatrix(pred_tree, test_tree$match)

# važnost varijabli
varImp(fit_tree)
```
---

## Usporedba modela (logistička vs kNN vs tree)

### Pitanje  
Koji model daje najbolju točnost i je li točnost vrijedna gubitka interpretabilnosti?**


```{r model-compare}
# Logistic regression kao ML baseline u caret-u
set.seed(123)

glm_ml_data <- data %>%
  select(match, attr, intel, fun, like, samerace) %>%
  drop_na() %>%
  mutate(match = factor(match, levels = c(0,1)))

idx <- createDataPartition(glm_ml_data$match, p = 0.8, list = FALSE)
train_glm <- glm_ml_data[idx, ]
test_glm  <- glm_ml_data[-idx, ]

ctrl <- trainControl(method = "cv", number = 5)

fit_glm_ml <- train(match ~ ., data = train_glm,
                    method = "glm",
                    family = binomial,
                    trControl = ctrl)

pred_glm <- predict(fit_glm_ml, newdata = test_glm)
cm_glm <- confusionMatrix(pred_glm, test_glm$match)

cm_knn <- confusionMatrix(pred_knn, test_knn$match)
cm_tree <- confusionMatrix(pred_tree, test_tree$match)

results <- tibble::tibble(
  model = c("Logistic (caret glm)", "kNN", "Decision tree"),
  accuracy = c(cm_glm$overall["Accuracy"],
               cm_knn$overall["Accuracy"],
               cm_tree$overall["Accuracy"])
)
results
```

